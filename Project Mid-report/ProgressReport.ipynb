{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Progress Report\n",
    "**Date:** April 10th, 2025\n",
    "\n",
    "**Team Members:**\n",
    "Alex Cruz,\n",
    "Miguel Madrigal,\n",
    "Christian Julias,\n",
    "Mohsin Patel,\n",
    "Angel Ramirez\n",
    "\n",
    "\n",
    "**Link to GitHub Repository:** https://github.com/mmadr5/SteamSalesAndPricingAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# ===========================================\n",
    "\n",
    "# 1. Project Introduction\n",
    "Briefly introduce your project, the dataset, and the problems you aim to investigate:\n",
    "\n",
    "The goal of our project is to analyze the pricing and discount strategies used on the popular PC gaming platform, Steam. \n",
    "As frequent gamers ourselves, we were particularly interested in understanding the patterns behind the widely anticipated Steam sales, \n",
    "and how various game attributes influence both regular prices and discount behavior.\n",
    "\n",
    "To do this, we collected data from several publicly available Steam-related datasets. These datasets include information on game prices, \n",
    "genres, user reviews, tags, release dates, and other game-specific metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# ===========================================\n",
    "\n",
    "# 2. Changes since Proposal\n",
    "Discuss any changes in the scope or approach since the initial proposal:\n",
    "\n",
    "Since our initial proposal, the scope and overall approach of our project have remained consistent. \n",
    "We haven't added or removed any major components. Our focus continues to be on analyzing Steam game pricing and discount patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===========================================\n",
    "\n",
    "# 3. Data Preparation\n",
    "Explain the steps taken to prepare your data.\n",
    "\n",
    "\n",
    "[Describe your data preparation process, including data cleaning, transformations, feature extraction, etc.]\n",
    "> Our dataset was collected from SteamDB where we first started with the top 100 games in each genre, just   because collecting\n",
    "data for the entire game catalog on Steam has been time consuming. To clean the data we looked at the price column and removed \n",
    "any entry that did not have a price or had a price of 0 which meant it was free, which is not needed for our project. After that we removed \n",
    "any duplicates or any rows with missing info like name or genre. \n",
    "These steps ensured our dataset was clean, consistent, and ready for analysis related to Steam's pricing patterns and sales strategy.\n",
    "\n",
    "> For Hypothesis 4, our dataset was collected from SteamDB. We originally curated 100 games across diverse genres and publisher types (AAA vs. Indie). However, due to time constraints and data availability, we focused on a subset of 50 games for collecting detailed price history and review history via downloading through their chart downloads. This is due to SteamDB requiring a valid Steam login with purchase history and frequent downloading causes timeouts and sometimes bans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Top 100 General Dataset (Used by Mohsin) ===\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('../Steam_Project/steam_top100_cleaned_data_corrected.csv')\n",
    "\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "display(df.head())\n",
    "\n",
    "# Dataset overview\n",
    "print(\"\\nData Info:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nStatistical Summary:\")\n",
    "display(df.describe())\n",
    "\n",
    "# Clean numeric price\n",
    "df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
    "df = df[(df['price'] > 0) & (df['price'] <= 210)]\n",
    "df.drop_duplicates(subset='appid', keep='first', inplace=True)\n",
    "\n",
    "# Add review ratio columns\n",
    "df['total_reviews'] = df['positive'] + df['negative']\n",
    "df['positive_ratio'] = df['positive'] / df['total_reviews'].replace(0, 1)\n",
    "df['negative_ratio'] = df['negative'] / df['total_reviews'].replace(0, 1)\n",
    "\n",
    "# Drop nulls and reset index\n",
    "df.dropna(subset=['name', 'genre'], inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"\\nCleaned Data Sample:\")\n",
    "display(df.head())\n",
    "\n",
    "# === Discount + Review Historical Data (Used by Miguel for Hyp 4, 5, ML2) ===\n",
    "review_folder = \"ReviewHistory\"\n",
    "price_folder = \"PriceHistory\"\n",
    "game_json = \"100_Steam_Assortment.json\"\n",
    "\n",
    "with open(game_json, \"r\", encoding=\"utf-8\") as f:\n",
    "    games = json.load(f)\n",
    "\n",
    "appid_type_map = {str(game[\"appid\"]): game[\"type\"] for game in games}\n",
    "records = []\n",
    "\n",
    "for appid in appid_type_map:\n",
    "    review_path = os.path.join(review_folder, f\"{appid}.csv\")\n",
    "    price_path = os.path.join(price_folder, f\"{appid}.csv\")\n",
    "\n",
    "    if not os.path.exists(review_path) or not os.path.exists(price_path):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        review_df = pd.read_csv(review_path)\n",
    "        price_df = pd.read_csv(price_path)\n",
    "\n",
    "        review_df.columns = [col.strip().lower() for col in review_df.columns]\n",
    "        price_df.columns = [col.strip().lower() for col in price_df.columns]\n",
    "\n",
    "        if \"final price\" not in price_df.columns or \"historical low\" not in price_df.columns:\n",
    "            continue\n",
    "\n",
    "        review_df[\"date\"] = pd.to_datetime(review_df[\"datetime\"]).dt.date\n",
    "        price_df[\"date\"] = pd.to_datetime(price_df[\"datetime\"]).dt.date\n",
    "\n",
    "        review_grouped = review_df.groupby(\"date\").agg({\n",
    "            \"positive reviews\": \"sum\",\n",
    "            \"negative reviews\": \"sum\"\n",
    "        }).reset_index()\n",
    "\n",
    "        price_grouped = price_df.groupby(\"date\").agg({\n",
    "            \"final price\": \"last\",\n",
    "            \"historical low\": \"last\"\n",
    "        }).reset_index()\n",
    "\n",
    "        merged = pd.merge(review_grouped, price_grouped, on=\"date\", how=\"inner\")\n",
    "        merged[\"discount_percent\"] = 100 * (merged[\"final price\"] - merged[\"historical low\"]) / merged[\"final price\"]\n",
    "        merged[\"discount_percent\"] = merged[\"discount_percent\"].clip(lower=0)\n",
    "        merged[\"review_volume\"] = merged[\"positive reviews\"] + merged[\"negative reviews\"]\n",
    "        merged = merged.dropna(subset=[\"discount_percent\", \"review_volume\"])\n",
    "        merged = merged[merged[\"review_volume\"] > 0]\n",
    "\n",
    "        for _, row in merged.iterrows():\n",
    "            records.append({\n",
    "                \"appid\": appid,\n",
    "                \"discount_percent\": row[\"discount_percent\"],\n",
    "                \"review_volume\": row[\"review_volume\"],\n",
    "                \"type\": appid_type_map[appid]\n",
    "            })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[{appid}] Error: {e}\")\n",
    "        continue\n",
    "\n",
    "# Final DataFrame used for Hypothesis 4, Hypothesis 5, and ML2\n",
    "df_discount_reviews = pd.DataFrame(records)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===========================================\n",
    "\n",
    "# 4. Exploratory Data Analysis (EDA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df['price'], bins=50)\n",
    "\n",
    "plt.title(\"Price Distribution of Apps\")\n",
    "plt.xlim(0, 210)\n",
    "plt.xlabel(\"Price\")\n",
    "plt.ylabel(\"Number of Apps\")\n",
    "plt.show()\n",
    "\n",
    "# Most common genres\n",
    "from collections import Counter\n",
    "\n",
    "all_genres = ','.join(df['genre'].dropna()).split(',')\n",
    "genre_counts = pd.Series(Counter(all_genres)).sort_values(ascending=False)\n",
    "genre_counts.head(10).plot(kind='bar')\n",
    "plt.title(\"Top 10 Most Common Genres\")\n",
    "plt.ylabel(\"Number of Apps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===========================================\n",
    "\n",
    "# 5. Hypotheses Visualizations\n",
    "(At least 5 visualizations with explanations and responsible team members)\n",
    "\n",
    "\n",
    "### Hypothesis 1:\n",
    "- Visualization explanation:\n",
    "\n",
    "    > The distribution of review counts (log-transformed) for only paid Steam apps is displayed in this violin plot.  The x-axis shows if a game's price is zero, although all entries here indicate paid titles (is_free = False), as all free games were filtered out. By lessening the impact of outliers, the log_reviews transformation (log(1 + total_reviews)) helps highlight patterns. With thicker portions showing more games in that category, the graphic displays the density of paid games across different review count ranges. The median and interquartile range are displayed in the box inside the violin, suggesting that the majority of paid games fall within a certain review volume. A long tail at the top indicates that some premium games get a disproportionately large number of reviews. Since free games are not included, we are better able to concentrate on trends in paid games without being swayed by popular free-to-play games. We are more aware of the variation in user engagement (as measured by reviews) in the paid market category thanks to this visualization.\n",
    "\n",
    "- Why it’s interesting:\n",
    "\n",
    "    > This visualization is interesting because it highlights the **uneven distribution of user engagement** among paid games on Steam. The quantity of reviews that the games receive varies greatly, despite the fact that they all cost money. A small percentage of paid games receive an excessively large number of reviews, whereas the majority just receive a low amount of attention. According to this long-tailed distribution, a few number of very successful games control user engagement, indicating that **success in the paid gaming market is not fairly shared**. Extreme outliers would otherwise hide these patterns, but the log scale makes them visible. It poses interesting considerations regarding the factors that influence popularity and exposure in the Steam marketplace. Additionally, since being a paid game does not ensure high interaction, it provides information into customer behavior. This plot helps developers and analysts evaluate player interest, spot market saturation, and plan their pricing and marketing strategies. In the end, it demonstrates that popularity among premium games is not merely uncommon, but unexpected.\n",
    "\n",
    "- Responsible member(s): Angel Ramirez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'is_free' column to indicate whether the app is free\n",
    "df['is_free'] = df['price'] == 0.00\n",
    "\n",
    "# Add 'log_reviews' column to calculate log(1 + total_reviews)\n",
    "import numpy as np\n",
    "df['log_reviews'] = np.log1p(df['total_reviews'])\n",
    "\n",
    "# Plot the violin plot\n",
    "sns.violinplot(x='is_free', y='log_reviews', data=df, inner='box')\n",
    "plt.title(\"Free vs Paid Apps: Log Review Count (Violin Plot)\")\n",
    "plt.xlabel(\"Is Free\")\n",
    "plt.ylabel(\"Log(1 + Total Reviews)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis 2:\n",
    "- Visualization explanation: \n",
    "\n",
    "    > The first visualization shows the top 10 game genres with the highest average positive review ratios. Each dot represents a genre, and its size reflects the number of games in that genre. Larger dots mean more games, giving context to how representative the average ratio is. The genres are sorted by positivity, helping us quickly see which genres tend to receive the most favorable feedback from players. For example, if “Indie” appears at the top, it means that on average, players rate Indie games very positively.\n",
    "\n",
    "    > The second visualization highlights the top 10 genres with the highest average negative review ratios. Like the first chart, dot size indicates how many games are in each genre. A high negative ratio suggests that games in that genre tend to receive more critical or dissatisfied reviews. This helps identify genres that may have issues with gameplay, quality, or audience expectations. Together, both plots give a balanced view of how different genres perform in terms of user sentiment. They can be used to compare genre reputation and player satisfaction patterns in the Steam game ecosystem.\n",
    "\n",
    "- Why is it intresting? \n",
    "\n",
    "    > It reveals which game genres are most loved or criticized by players on average. It helps developers and gamers understand trends in player satisfaction across different genres. By comparing both positive and negative feedback, we can spot genres that may be underrated or overhyped. It also offers insight into how genre popularity relates to review quality, not just quantity.\n",
    "\n",
    "- Responsible member(s): Alexander Cruz \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Visualization Code]\n",
    "# Sort and select top 10 by positive ratio\n",
    "\n",
    "# Extract main genre\n",
    "df['main_genre'] = df['genre'].str.split(',').str[0]\n",
    "\n",
    "# Group by main genre and compute averages\n",
    "genre_summary = df.groupby('main_genre').agg(\n",
    "    avg_positive_ratio=('positive_ratio', 'mean'),\n",
    "    avg_negative_ratio=('negative_ratio', 'mean'),\n",
    "    app_count=('appid', 'count')\n",
    ").sort_values(by='avg_positive_ratio', ascending=False).head(10)\n",
    "\n",
    "top_positive = genre_summary.sort_values(by='avg_positive_ratio', ascending=False).head(10)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(\n",
    "    top_positive['avg_positive_ratio'],\n",
    "    top_positive.index,\n",
    "    s=top_positive['app_count'] * 3,\n",
    "    alpha=0.7,\n",
    "    color='green'\n",
    ")\n",
    "plt.title(\"Top 10 Genres by Average Positive Review Ratio\")\n",
    "plt.xlabel(\"Average Positive Ratio\")\n",
    "plt.ylabel(\"Main Genre\")\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Sort and select top 10 by negative ratio\n",
    "top_negative = genre_summary.sort_values(by='avg_negative_ratio', ascending=False).head(10)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(\n",
    "    top_negative['avg_negative_ratio'],\n",
    "    top_negative.index,\n",
    "    s=top_negative['app_count'] * 3,\n",
    "    alpha=0.7,\n",
    "    color='red'\n",
    ")\n",
    "plt.title(\"Top 10 Genres by Average Negative Review Ratio\")\n",
    "plt.xlabel(\"Average Negative Ratio\")\n",
    "plt.ylabel(\"Main Genre\")\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis 3:\n",
    "- Visualization explanation: \n",
    "\n",
    "    > This visualization shows us the box plots of the positive review ratio of 7 different price ranges of games. The price ranges are $0.25-$1, $1-$5, 5$-10$, 10$-20$, 20$-50$, 50$-100$, and $100+.\n",
    "    > From the price range of 0.25-1 we find that the spread of positive reviews ranges from 0 to 1.0. The median is around 0.75. The interquartile range is from about 0.5 to 0.9. The spread is already wide so we dont see many outliers.\n",
    "    > From the price range of 1-5 we find that the spread of positive reviews ranges from 0.1 to 1.0. The median is around 0.80. The interquartile range is from about 0.6 to 0.95. We are now seeing more outliers start to show at the 0.0 - 0.1 positive review ratio.\n",
    "    > From the price ranges of 5-10, 10-20, and 20-50 we see a similar spread of data from about 0.35 to 1.0. The median is consistent around 0.80. The interquartile range is also consistent around 0.70 to 0.90. We also see lots of outliers on the lower end of the ratings ranging from 0.0 - 0.4.\n",
    "    > From the ranges of 50-100 we see the same outcome as the 5-50 price range except this time there are far less outliers especially at the really low end.\n",
    "    > The 100+ price range is where the data really differs. The spread of data is spread wide from 0.0 to 1.0. The median rating is the lowest at just 0.4. The interquartile range is from 0.3 - 0.7. The spread is already wide so we dont see many outliers.\n",
    "\n",
    "- Why is it interesting? \n",
    "\n",
    "    > This is interesting because it shows us how much cost really impacts the positive reviews that people leave on games. For the 0.25-1 price range there is a wide spread of reviews because there really isn't consistent quality in cheaper games. Since people aren't spending that much money ratings for these games are still quite high. When games cost more money (around the 1-50 price range) we start to see more outliers meaning that users are likely to leave lower reviews if they spend more money on the game they bought. The median still remains quite high for these games however. The 50-100 price range is typically where AAA games are priced and the median positive review ratio still remained high but we actually see less outliers compared to the games priced in the 5-50 range which is surprising. At the 100+ price range we see the lowest positive reviews with a median at 0.4 and a really wide spread of data. This is a considerable amount of money to spend on a game and users are less likely to leave a positive review as a result.\n",
    "\n",
    "- Responsible member(s): Mohsin Patel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Visualization Code]\n",
    "# Create price bins\n",
    "df['price_bin'] = pd.cut(df['price'], bins=[0, 1, 5, 10, 20, 50, 100, 200], labels=['$0.25-1', '$1-5', '$5-10', '$10-20', '$20-50', '$50-100', \"$100+\"], include_lowest=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='price_bin', y='positive_ratio', data=df)\n",
    "plt.title(\"Positive Review Ratio by Price Range\")\n",
    "plt.xlabel(\"Price Range\")\n",
    "plt.ylabel(\"Positive Review Ratio\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis 4:\n",
    "- Visualization explanation: \n",
    "\n",
    "    > The scatter plot visualizes the relationship between discount percentage and daily review volume for 50 selected games. Points are color-coded by game type (AAA vs Indie). The x-axis represents the discount percentage applied on a given day, while the y-axis shows the number of user reviews (positive + negative) submitted that same day. A regression line is plotted to estimate overall correlation.\n",
    "\n",
    "    > The chart suggests that heavier discounts (e.g., 40–80%) are often correlated with higher spikes in review volume, particularly for AAA games. Indie games show review spikes more uniformly across all discount levels, including moderate discounts (10–30%). Some games like PUBG became free so reviews were accounted on the 100% discount.\n",
    "\n",
    "- Why is it intresting? \n",
    "\n",
    "    > This visualization is crucial for understanding how player engagement, reflected by review frequency, responds to price changes. It provides evidence that sales events can drive more player feedback, especially for major titles. Moreover, the difference in trend between AAA and indie games suggests that consumer sensitivity to discounts varies by market segment.\n",
    "\n",
    "    > The plot helps support the idea that sales periods represent key touchpoints for community interaction, and that publishers can potentially use discounting strategies to prompt player attention and word-of-mouth via Steam reviews. Think of your favorite YouTubers and Twitch Streamers talking about that new game or exciting game that is now on sale. \n",
    "\n",
    "    > This finding lays the groundwork for more granular analysis, such as whether the sentiment of those reviews changes significantly during sales. However, we need more games to improve this analysis\n",
    "\n",
    "    > However, the regression model will show that discount percentage alone is not a strong indicator of review volume.\n",
    "\n",
    "- Responsible member(s): Miguel Madrigal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Visualization Code]\n",
    "if df_discount_reviews.empty:\n",
    "    print(\"No data to visualize.\")\n",
    "else:\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    sns.scatterplot(\n",
    "        data=df_discount_reviews,\n",
    "        x=\"discount_percent\",\n",
    "        y=\"review_volume\",\n",
    "        hue=\"type\",\n",
    "        alpha=0.6,\n",
    "        palette=\"Set1\"\n",
    "    )\n",
    "    sns.regplot(\n",
    "        data=df_discount_reviews,\n",
    "        x=\"discount_percent\",\n",
    "        y=\"review_volume\",\n",
    "        scatter=False,\n",
    "        lowess=True,\n",
    "        color=\"black\"\n",
    "    )\n",
    "\n",
    "    plt.title(\"Hypothesis 4: Do Discounts Correlate with Review Volume?\")\n",
    "    plt.xlabel(\"Discount Percentage (%)\")\n",
    "    plt.ylabel(\"Review Volume (per day)\")\n",
    "    plt.legend(title=\"Game Type\")\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis 5:\n",
    "- Visualization explanation:\n",
    "> The violin plot compares the discount frequency between AAA and Indie games using the discount ratio (days discounted ÷ total days observed). Each violin shape represents the distribution of discount ratios within each game type. Wider sections indicate more games falling around that ratio.\n",
    "From the plot, Indie games generally show higher discount frequency, clustering around 70%–90% of the time discounted. In contrast, AAA games show more variability, with some frequently discounted and others much less so. This supports the idea that Indie titles rely more consistently on discounting as a marketing strategy, while AAA games are more selective.\n",
    "\n",
    "\n",
    "- Responsible member(s): Miguel Madrigal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON game list\n",
    "with open(\"100_Steam_Assortment.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    games = json.load(f)\n",
    "\n",
    "appid_type_map = {str(game[\"appid\"]): game[\"type\"] for game in games}\n",
    "\n",
    "# Paths to historical price data\n",
    "price_folder = \"PriceHistory\"\n",
    "discount_freq = {\"AAA\": [], \"Indie\": []}\n",
    "\n",
    "# Loop through appids and count discount days\n",
    "for appid, gtype in appid_type_map.items():\n",
    "    path = os.path.join(price_folder, f\"{appid}.csv\")\n",
    "    if not os.path.exists(path):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        df.columns = [col.strip().lower() for col in df.columns]\n",
    "        if \"final price\" not in df.columns or \"historical low\" not in df.columns:\n",
    "            continue\n",
    "\n",
    "        df[\"final price\"] = pd.to_numeric(df[\"final price\"], errors=\"coerce\")\n",
    "        df[\"historical low\"] = pd.to_numeric(df[\"historical low\"], errors=\"coerce\")\n",
    "\n",
    "        # A discount is when historical low < final price\n",
    "        df[\"is_discounted\"] = df[\"historical low\"] < df[\"final price\"]\n",
    "        num_discount_days = df[\"is_discounted\"].sum()\n",
    "        total_days = len(df)\n",
    "\n",
    "        if total_days > 0:\n",
    "            discount_ratio = num_discount_days / total_days\n",
    "            discount_freq[gtype].append(discount_ratio)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing AppID {appid}: {e}\")\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "discount_df = pd.DataFrame([\n",
    "    {\"Type\": \"AAA\", \"Discount Ratio\": ratio} for ratio in discount_freq[\"AAA\"]\n",
    "] + [\n",
    "    {\"Type\": \"Indie\", \"Discount Ratio\": ratio} for ratio in discount_freq[\"Indie\"]\n",
    "])\n",
    "\n",
    "# Plot the distributions\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.violinplot(data=discount_df, x=\"Type\", y=\"Discount Ratio\", inner=\"box\", palette=\"Set2\")\n",
    "plt.title(\"Distribution of Discount Frequency by Game Type\")\n",
    "plt.ylabel(\"Discount Ratio (Days Discounted / Total Days)\")\n",
    "plt.xlabel(\"Game Type\")\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Machine Learning Analyses\n",
    "(At least 2 analyses with baselines and explanations)\n",
    "\n",
    "**Analysis 1:**\n",
    "- ML technique explanation:\n",
    "  > We used logistic regression to model the probability of a binary outcome happening. We want to predict when a game is highly rated or not. The inputs we use are the price of the game and the total reviews the game has received.\n",
    "- Baseline used:\n",
    "  > The baseline used would be predicting that all games are highly rated since most ratings are skewed to being positive.\n",
    "- Results interpretation:\n",
    "  > Our model is biased to predicting games are highly rated because the majority class is true. It struggles to predict when the result is false. This tells us that we may need a bigger dataset with different features so that we have different features to help predict whether a game is highly rated or not since price and total reviews alone does not do a great job at helping the model predict correctly.\n",
    "- Responsible member(s): Mohsin Patel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ML Analysis Code]\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "df['is_highly_rated'] = df['positive_ratio'] >= 0.8\n",
    "\n",
    "X = df[['price', 'total_reviews']]\n",
    "y = df['is_highly_rated']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis 2:**\n",
    "- ML technique explanation:\n",
    "> We used simple linear regression to examine whether a game’s daily discount percentage can predict its daily review volume. This involved merging Steam price history and review history across 50 games, calculating per-day discount percentages, and summing positive and negative reviews to get total review volume.\n",
    "- Baseline used:\n",
    "> Our baseline is the assumption that review volume stays constant regardless of discounts. We compare our regression model’s performance to this by using R²\n",
    "- Results interpretation:\n",
    "> The model produced an R² score of 0.0145 and a mean absolute error of 162.07.\n",
    "This means discount percentage alone explains less than 2% of the variation in review volume, and the model's predictions are off by about 162 reviews per day on average.\n",
    "These results indicate that discounts may influence review activity but are not a strong standalone predictor. Other variables like game popularity, update cycles, and Steam events likely play a larger role.\n",
    "- Responsible member(s): Miguel Madrigal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "# Use already-prepared df_discount_reviews\n",
    "df = df_discount_reviews.copy()\n",
    "\n",
    "# Machine Learning input/output\n",
    "X = df[[\"discount_percent\"]]\n",
    "y = df[\"review_volume\"]\n",
    "\n",
    "# Train-test split and model training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = LinearRegression().fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Output performance\n",
    "print(\"Linear Regression Performance:\")\n",
    "print(f\"R² Score: {r2_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error(y_test, y_pred):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===========================================\n",
    "\n",
    "# 7. Reflection\n",
    "*Address the reflection points:*\n",
    "\n",
    "- **Most challenging part so far:** Gathering data or finding useful consistent data has been a nightmare\n",
    "\n",
    "- **Initial insights:**\n",
    ">    So far, we’ve observed that genre appears to play a notable role in pricing trends—certain genres tend to have higher average base prices and steeper discounts during major sales. User reviews and release dates also correlate interestingly with discount frequency and size. Early data exploration suggests that newer games receive fewer discounts initially, while older, positively reviewed games are more likely to be discounted heavily over time. These trends could help identify strategic pricing windows for developers or highlight what types of games benefit most from Steam’s promotional events.\n",
    "\n",
    "- **Concrete results available:**\n",
    ">    Cleaned and merged datasets from SteamDB, focusing on top 100 games per genre\n",
    "Removed entries with free pricing or missing values to ensure more accurate analysis\n",
    "Grouped and visualized average prices and discount rates across genres\n",
    "Identified initial relationships between user review scores and price fluctuation\n",
    "Created a GitHub repository to host code and visuals, which is actively updated by the team\n",
    "\n",
    "- **Current biggest problems:**\n",
    ">    One major challenge has been the time-intensive nature of data collection, especially considering the vast size of the Steam catalog. We opted to limit our dataset to the top 100 games per genre to make the analysis more manageable. Another issue is ensuring consistency in tags and genre classifications, as some games fall into multiple overlapping categories. Lastly, we're still refining how to best represent discounts over time—especially for games with inconsistent or irregular pricing histories.\n",
    "\n",
    "- *Are you on track?*\n",
    ">    Yes, as a group we’re generally on track. We’ve had to navigate a few scheduling challenges and coordinate around different working styles, but we’ve maintained steady progress overall. We’ve divided responsibilities based on each member’s strengths and have been communicating regularly to ensure alignment. While there’s always room for improvement, we’re confident that we’re moving in the right direction and are on pace to meet our goals within the timeline.\n",
    "\n",
    "- *Worth proceeding with the current approach?*\n",
    ">    Yes, our current approach seems to be working well so far. Focusing on the top 100 games per genre has made the analysis more manageable while still giving us solid insights. We've started to notice clear patterns in how pricing and discounts vary across genres and based on reviews, which supports our original goals. There are still areas we’re refining, but overall, the direction we’re taking feels effective and worth continuing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===========================================\n",
    "\n",
    "# 8. Next Steps\n",
    "Outline concrete plans and goals for the next month.\n",
    "\n",
    "- In the last week we found a dataset table that lists the games currently on steam. We hope to use this table to retrieve current price and review data.\n",
    "- We recently retrieved a table that has listed publisher data classed by AAA, AA, Indie, and Hobbyist. As well other relevant data per publisher that we can leverage in our study.\n",
    "- [Goal 3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
